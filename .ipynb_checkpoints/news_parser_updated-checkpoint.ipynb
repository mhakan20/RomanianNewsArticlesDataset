{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15b2c688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /Users/hakanmeva/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages (4.11.1)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/hakanmeva/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from beautifulsoup4) (2.3.2.post1)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f434d056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-12.0.1-cp39-cp39-macosx_11_0_arm64.whl (22.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.7/22.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /Users/hakanmeva/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from pyarrow) (1.23.2)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-12.0.1\n"
     ]
    }
   ],
   "source": [
    "! pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53ad70bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cd43236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdf3b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parser(url):\n",
    "    headers = {'User-agent': 'Chrome/108.0.0.0'} \n",
    "    request = requests.get(url, headers=headers) \n",
    "    html = request.content\n",
    "    parser = BeautifulSoup(html, 'html.parser') \n",
    "    \n",
    "    return parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e9aced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_parser = create_parser('https://www.zf.ro/finante-personale/arhiva/2022/01/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f47a852",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on month: 01\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on month: 02\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 03\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on month: 04\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on page: 5\n",
      "Working on month: 05\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 06\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 07\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on month: 08\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on month: 09\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 10\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 11\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 12\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "CPU times: user 1min 30s, sys: 3.06 s, total: 1min 33s\n",
      "Wall time: 8min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#21:08 start\n",
    "news_df = pd.DataFrame()\n",
    "months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "#months = ['04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "for month in months:\n",
    "    print('Working on month: {}'.format(month))\n",
    "    init_parser = create_parser('https://www.zf.ro/finante-personale/arhiva/2020/' + month + '/')\n",
    "    max_page = 1\n",
    "    for parsed in init_parser.find_all(\"li\"):\n",
    "        if 'title' in parsed.find_next('a').attrs:\n",
    "            if 'pagina' in parsed.find_next('a')['title']:\n",
    "                if int(parsed.find_next('a')['title'][-1:]) > max_page:\n",
    "                    max_page = int(parsed.find_next('a')['title'][-1:])\n",
    "    for i in range(1, max_page+1):\n",
    "        print('Working on page: {}'.format(i))\n",
    "        \n",
    "        parser = create_parser('https://www.zf.ro/finante-personale/arhiva/2020/'+ month + '/page/' + str(i))\n",
    "        \n",
    "        for article_h2 in parser.findAll('h2'):\n",
    "            href = article_h2.find_next('a', href=True)['href']\n",
    "            title = article_h2.find_next('a', href=True)\\\n",
    "                                          .contents[0]\\\n",
    "                                          .get_text()\\\n",
    "                                          .strip()\n",
    "            try:\n",
    "                parser_article = create_parser('https://www.zf.ro' + article_h2.find_next('a', href=True)['href'])\n",
    "\n",
    "                json_scr = json.loads(parser_article.find_all('script')[2].text, strict = False)\n",
    "                date_published = json_scr['datePublished']\n",
    "                headline = json_scr['headline']\n",
    "                article_section = json_scr['articleSection']\n",
    "                description = json_scr['description']\n",
    "                if 'name' in json_scr['author']:\n",
    "                    author_name = json_scr['author']['name']\n",
    "                else:\n",
    "                    author_name = None\n",
    "                article_text = json_scr['articleBody']\n",
    "                rec_refs = parser_article.find_all(\"a\", {\"class\": \"thumb picture-type\"})\n",
    "                rec_hrefs = []\n",
    "                for ref in range(len(rec_refs)):\n",
    "                    rec_hrefs.append(rec_refs[ref]['href'])\n",
    "\n",
    "                dict_append = {'href': href,\n",
    "                              'title': title,\n",
    "                              'date_published': date_published,\n",
    "                              'headline': headline,\n",
    "                              'article_section': article_section,\n",
    "                              'description': description,\n",
    "                              'author_name': author_name,\n",
    "                              'article_text': article_text,\n",
    "                              'rec_hrefs': rec_hrefs}\n",
    "                news_df = news_df.append(dict_append, ignore_index = True)\n",
    "            except:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "165cdeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.to_parquet('datasets/df_finante_pers_2020.parquet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17b90fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_parquet('datasets/df_finante_pers_2020.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
